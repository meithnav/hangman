{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import ast\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_INDEX=2\n",
    "# GPU_INDEX = \"2,4,5\"\n",
    "\n",
    "\n",
    "isGPU = True\n",
    "\n",
    "\n",
    "VERSION=0\n",
    "\n",
    "\n",
    "# isTEST='test-'\n",
    "isTEST=''\n",
    "\n",
    "\n",
    "BATCH_SIZE=256 \n",
    "EPOCHS=15\n",
    "\n",
    "MODEL_NAME = \"roberta\"\n",
    "# MODEL_NAME = \"bart\"\n",
    "\n",
    "\n",
    "MAX_LEN = 31 \n",
    "PAD_TOKEN = '[PAD]'\n",
    "MASK_TOKEN = '_'\n",
    "BOS_TOKEN = \"[START]\"\n",
    "EOS_TOKEN = \"[END]\"\n",
    "\n",
    "# Define special tokens\n",
    "SPECIAL_TOKENS = {\"pad_token\": PAD_TOKEN, \"bos_token\": BOS_TOKEN, \"eos_token\":EOS_TOKEN, 'mask_token':MASK_TOKEN }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF model\n",
    "# HF_API_TOKEN = os.getenv(\"HF_API_TOKEN\")\n",
    "\n",
    "\n",
    "if isGPU:\n",
    "    # os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" ## to avoid Context Switching \n",
    "    os.environ[\"HF_HOME\"]= \"/data2/meithnav/.hfcache/\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(GPU_INDEX) # not changing GPU. Only\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaForMaskedLM, Trainer, TrainingArguments, BartForConditionalGeneration, DefaultDataCollator\n",
    "\n",
    "if isGPU:\n",
    "    # torch.cuda.set_device(0) ## setgpu\n",
    "    # print(\"\\n\\n--> CONNECTED TO GPU NO: \", torch.cuda.current_device())\n",
    "    print(\"--> GPU_INDEX: \", GPU_INDEX)\n",
    "        \n",
    "    # GPU (MPS for Apple Silicon, CUDA for Nvidia GPUs, or CPU)\n",
    "\n",
    "    torch.cuda.empty_cache() # clear GPU cache\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# APPEND ROOT DIRECTORY\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('hangman'), '..')))\n",
    "\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZER & DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "  \n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read all lines, strip whitespace, and filter out empty lines\n",
    "        vocabulary = [line.strip() for line in file if line.strip()]\n",
    "    return vocabulary \n",
    "\n",
    "\n",
    "def load_and_convert_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        # Read each line and parse the stringified JSON into a Python dict\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    # Convert the list of dicts into a Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_list(val):\n",
    "    return ast.literal_eval(val)\n",
    "\n",
    "\n",
    "# Function to apply the conversion using multithreading\n",
    "def load_parallel_dataframe_apply(df, func, n_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Apply the function in parallel for 'input' and 'target' columns\n",
    "        input_data = list(executor.map(func, df['input']))\n",
    "        target_data = list(executor.map(func, df['target']))\n",
    "    \n",
    "    return input_data, target_data\n",
    "\n",
    "\n",
    "# Function to process a single line of JSON data and convert it to a dictionary\n",
    "def process_json_line(line):\n",
    "    return json.loads(line)\n",
    "\n",
    "# Function to load the JSON data in parallel\n",
    "def load_json_in_parallel(file_path, n_workers=4):\n",
    "    # Read the file line by line\n",
    "    print(f\"\\n  =>LOADING {file_path}\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelize the JSON parsing\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        data = list(executor.map(process_json_line, lines))\n",
    "    \n",
    "   \n",
    "    # Convert the list of dictionaries into a pandas DataFrame\n",
    "    df= pd.DataFrame(data[0])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer \n",
    "class CharLevelTokenizer:\n",
    "    def __init__(self, vocab, special_tokens):\n",
    "        self.char_vocab = vocab\n",
    "        self.char_to_id = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.id_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_id[char] for char in text]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        ignore_tokens = ['[PAD]', '_', '[START]', '[END]' ]\n",
    "        ignore_idx = [self.char_to_id[token] for token in ignore_tokens]\n",
    "        return \"\".join([ self.id_to_char[token_id] if token_id not in ignore_idx else '' for token_id in token_ids ])\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\"Implement a save method that writes the tokenizer data to disk.\"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        # Save the vocab\n",
    "        with open(os.path.join(save_directory, 'vocab.json'), 'w') as f:\n",
    "            json.dump(self.char_vocab, f)\n",
    "        \n",
    "\n",
    "    def from_pretrained(self, pretrained_directory):\n",
    "        \"\"\"Load the tokenizer from a saved directory.\"\"\"\n",
    "        with open(os.path.join(pretrained_directory, 'vocab.json'), 'r') as f:\n",
    "            self.char_vocab = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Use the default data collator or ensure tensors are on CPU\n",
    "data_collator = DefaultDataCollator(return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLevelDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, special_tokens, max_length=29, device='cpu'):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.special_tokens = special_tokens\n",
    "        self.max_length = max_length\n",
    "        self.device = device \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get input and target text at the given index\n",
    "        input_text = self.inputs.iloc[idx] if isinstance(self.inputs, pd.Series) else self.inputs[idx]\n",
    "        target_text = self.targets.iloc[idx] if isinstance(self.targets, pd.Series) else self.targets[idx]\n",
    "\n",
    "        # Tokenize input and target text\n",
    "        input_ids = self.tokenizer.encode(input_text)[:self.max_length]  # Truncate to max_length\n",
    "        target_ids = self.tokenizer.encode(target_text)[:self.max_length]  # Truncate to max_length\n",
    "\n",
    "        # Convert to tensors and move to the specified device\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(target_ids, dtype=torch.long),  # Use \"labels\" instead of \"target_ids\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_model_for_char_vocab(model_type, char_vocab):\n",
    "   \n",
    "    if model_type == \"roberta\":\n",
    "        model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "    elif model_type == \"bart\":\n",
    "        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type. Choose 'roberta' or 'bart'.\")\n",
    "\n",
    "    # Resize token embeddings to match character-level vocabulary size\n",
    "    model.resize_token_embeddings(len(char_vocab))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example input\n",
    "# input_data = [[\"_\", \"_\", \"_\"], [\"a\", \"_\", \"_\"], [\"_\", \"a\", \"_\"], [\"_\", \"_\", \"b\"], \n",
    "#               [\"a\", \"a\", \"_\"], [\"a\", \"_\", \"b\"], [\"_\", \"a\", \"b\"], [\"a\", \"a\", \"b\"]]\n",
    "\n",
    "# # Target for prediction\n",
    "# target_data = [[\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"],\n",
    "#                [\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## LOADING DATASET\")\n",
    "\n",
    "\n",
    "## Load dataset [DF]\n",
    "# df = pd.read_csv(f\"./datasets/{isTEST}dataset.csv\", sep='|') \n",
    "# print(\"DATASET SHAPE : \", df.shape)\n",
    "\n",
    "# input_data  = df['input'].apply(ast.literal_eval)\n",
    "# target_data = df['target'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "# # Apply parallel processing to 'input' and 'target' columns\n",
    "# input_data, target_data = load_parallel_dataframe_apply(df, convert_to_list, n_workers=8)\n",
    "\n",
    "\n",
    "## LOAD Dataset [json]\n",
    "start = time.time()\n",
    "df = load_json_in_parallel(f\"./datasets/{isTEST}dataset.json\" , n_workers=8)\n",
    "input_data  = df.input\n",
    "target_data = df.target\n",
    "NUM_SAMPLES = df.shape[0]\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\" =>TIME SPENT : {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer to tokenize by lowercase characters only\n",
    "char_vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"[PAD]\", \"_\", \"[START]\", \"[END]\"]\n",
    "\n",
    "\n",
    "char_tokenizer = CharLevelTokenizer(char_vocab, SPECIAL_TOKENS)\n",
    "\n",
    "mask_token_id = char_tokenizer.char_to_id[SPECIAL_TOKENS['mask_token']]\n",
    "pad_token_id = char_tokenizer.char_to_id[SPECIAL_TOKENS['pad_token']]\n",
    "start_token_id = char_tokenizer.char_to_id[SPECIAL_TOKENS['bos_token']]\n",
    "end_token_id = char_tokenizer.char_to_id[SPECIAL_TOKENS['eos_token']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_input_data, test_input_data, train_target_data, test_target_data = train_test_split(\n",
    "    input_data, target_data, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize dataset\n",
    "train_dataset = CharLevelDataset(train_input_data, train_target_data, char_tokenizer, SPECIAL_TOKENS, MAX_LEN, device)\n",
    "test_dataset = CharLevelDataset(test_input_data, test_target_data, char_tokenizer, SPECIAL_TOKENS, MAX_LEN, device)\n",
    "\n",
    "\n",
    "# Initialize dataloader\n",
    "# train_dataset_loader = DataLoader(train_dataset, batch_size=64, collate_fn=char_level_data_collator)\n",
    "# test_dataset_loader = DataLoader(test_dataset, batch_size=64, collate_fn=char_level_data_collator)\n",
    "\n",
    "\n",
    "# for batch in train_dataset_loader:  # Correctly iterates over batches\n",
    "#     print(\"Input IDs:\", batch[\"input_ids\"])\n",
    "#     print(\"Target IDs:\", batch[\"target_ids\"])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==> LOADING MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load pre-trained RoBERTa model for Masked Language Modeling (MLM)\n",
    "# model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# # Resize the model's token embeddings to match the character-level vocab size\n",
    "# model.resize_token_embeddings(len(char_tokenizer.char_vocab))  # Resize for lowercase char-level tokens\n",
    "\n",
    "model = prepare_model_for_char_vocab(MODEL_NAME, char_tokenizer.char_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Move model to MPS\n",
    "model.to(device)\n",
    "\n",
    "# Setup the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./models/model-{MODEL_NAME}-EP_{EPOCHS}-{VERSION}/results', \n",
    "    # evaluation_strategy=\"steps\", \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=min(BATCH_SIZE, int(NUM_SAMPLES/4)), \n",
    "    logging_dir=f'./models/model-{MODEL_NAME}-EP_{EPOCHS}-{VERSION}/logs', \n",
    "    logging_steps=50 if isTEST else 10000,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=char_tokenizer,  \n",
    "    # data_collator=None,  \n",
    "    data_collator=data_collator, \n",
    ")\n",
    "\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(f\"./models/model-{MODEL_NAME}-EP_{EPOCHS}-{VERSION}/fine_tuned_{MODEL_NAME}\")\n",
    "print(\"\\n==> MODEL SAVED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_characters(input_sequence, tokenizer, model, tokens, max_len=30):\n",
    "    \n",
    "    # pad tokens\n",
    "    input_sequence = [tokens['bos_token']] + input_sequence + [tokens['eos_token']] + [tokens['pad_token']]*(max_len - len(input_sequence) - 2)\n",
    "\n",
    "    # print(input_sequence)\n",
    "    \n",
    "    # Convert the input sequence to token IDs using the tokenizer\n",
    "    input_ids = tokenizer.encode(input_sequence)\n",
    "\n",
    "    # Convert input_ids to tensor and move it to the right device\n",
    "    input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "    # Run the model to predict masked token positions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Extract the predicted token IDs for each masked position\n",
    "    predicted_ids = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Convert predicted IDs to characters using the tokenizer\n",
    "    predicted_sequence = tokenizer.decode(predicted_ids)\n",
    "    return predicted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFinetunedModel(model_name , model_path = None):\n",
    "\n",
    "    if model_path and model_name == \"roberta\":\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "    elif model_path and model_name == \"bart\":\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    else:\n",
    "        print(\"ENTER VALID MODEL PATH!!\")\n",
    "        raise ValueError(\"Unsupported model type. Choose 'roberta' or 'bart'.\")\n",
    "\n",
    "\n",
    "    # Set the model to evaluation mode for inference\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"./models/model-{MODEL_NAME}-EP_{EPOCHS}-{VERSION}/fine_tuned_{MODEL_NAME}\"\n",
    "model = loadFinetunedModel(MODEL_NAME, model_path)\n",
    "\n",
    "# Load the tokenizer (same tokenizer used during training)\n",
    "char_tokenizer = CharLevelTokenizer(char_vocab, SPECIAL_TOKENS)\n",
    "\n",
    "\n",
    "mask_token_id = char_tokenizer.char_to_id[SPECIAL_TOKENS['mask_token']]\n",
    "pad_token_id = char_tokenizer.char_to_id[SPECIAL_TOKENS['pad_token']]\n",
    "start_token_id = char_tokenizer.char_to_id[SPECIAL_TOKENS['bos_token']]\n",
    "end_token_id = char_tokenizer.char_to_id[SPECIAL_TOKENS['eos_token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example masked input\n",
    "print(\"\\n==> TESTNG INFERENCE\")\n",
    "\n",
    "# masked_input = [\"_\", \"a\", \"_\", \"s\"]\n",
    "# answer = \"plays\"\n",
    "\n",
    "masked_input = [\"a\", \"a\", \"_\", \"_\", \"e\", \"_\"]\n",
    "answer = \"aachen\"\n",
    "\n",
    "\n",
    "# Call the prediction function\n",
    "predicted_output = predict_masked_characters(masked_input, char_tokenizer, model, SPECIAL_TOKENS, MAX_LEN)\n",
    "\n",
    "print(f\"Predicted Output: {predicted_output}\\nANSWER: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MODEL PATH : ./models/model-{MODEL_NAME}-EP_{EPOCHS}-{VERSION}/fine_tuned_{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # clear GPU cache\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "print(f\"FINE-TUNED {MODEL_NAME}, GPU: {GPU_INDEX}\")\n",
    "\n",
    "print(\"\\n\\n ****ENDED SESSION !!*** \\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "domianins",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
