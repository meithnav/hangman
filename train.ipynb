{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, Trainer, TrainingArguments, BartForConditionalGeneration\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import time\n",
    "import ast\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION=1\n",
    "\n",
    "\n",
    "# isTEST='test-'\n",
    "isTEST=''\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "MAX_LEN=29\n",
    "\n",
    "\n",
    "# MODEL_NAME = \"roberta\"\n",
    "MODEL_NAME = \"bart\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZER & DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "  \n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read all lines, strip whitespace, and filter out empty lines\n",
    "        vocabulary = [line.strip() for line in file if line.strip()]\n",
    "    return vocabulary \n",
    "\n",
    "\n",
    "def load_and_convert_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        # Read each line and parse the stringified JSON into a Python dict\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    # Convert the list of dicts into a Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_list(val):\n",
    "    return ast.literal_eval(val)\n",
    "\n",
    "\n",
    "# Function to apply the conversion using multithreading\n",
    "def load_parallel_dataframe_apply(df, func, n_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Apply the function in parallel for 'input' and 'target' columns\n",
    "        input_data = list(executor.map(func, df['input']))\n",
    "        target_data = list(executor.map(func, df['target']))\n",
    "    \n",
    "    return input_data, target_data\n",
    "\n",
    "\n",
    "# Function to process a single line of JSON data and convert it to a dictionary\n",
    "def process_json_line(line):\n",
    "    return json.loads(line)\n",
    "\n",
    "# Function to load the JSON data in parallel\n",
    "def load_json_in_parallel(file_path, n_workers=4):\n",
    "    # Read the file line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelize the JSON parsing\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        data = list(executor.map(process_json_line, lines))\n",
    "    \n",
    "   \n",
    "    # Convert the list of dictionaries into a pandas DataFrame\n",
    "    df= pd.DataFrame(data[0])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer \n",
    "class CharLevelTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.char_vocab = vocab\n",
    "        self.char_to_id = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.id_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_id[char] if char in self.char_to_id else self.char_to_id[\"_\"] for char in text]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return \"\".join([self.id_to_char[token_id] for token_id in token_ids])\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\"Implement a save method that writes the tokenizer data to disk.\"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        # Save the vocab\n",
    "        with open(os.path.join(save_directory, 'vocab.json'), 'w') as f:\n",
    "            json.dump(self.char_vocab, f)\n",
    "        \n",
    "        # # Optionally, save any other tokenizer-related data\n",
    "        # # For example, special tokens file, etc.\n",
    "        # special_tokens = {'pad_token': self.pad_token_id, 'mask_token': self.mask_token}\n",
    "        # with open(os.path.join(save_directory, 'special_tokens_map.json'), 'w') as f:\n",
    "        #     json.dump(special_tokens, f)\n",
    "\n",
    "    def from_pretrained(self, pretrained_directory):\n",
    "        \"\"\"Load the tokenizer from a saved directory.\"\"\"\n",
    "        with open(os.path.join(pretrained_directory, 'vocab.json'), 'r') as f:\n",
    "            self.char_vocab = json.load(f)\n",
    "        # Optionally, load other data like special tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def char_level_data_collator(batch, MAX_LEN, device, pad_token_id):\n",
    "\n",
    "#     input_ids = []\n",
    "#     target_ids = []\n",
    "#     for item in batch:\n",
    "#         # Pad sequences to the max length in the batch\n",
    "#         input_ids.append(item[\"input_ids\"] + [pad_token_id] * (MAX_LEN - len(item[\"input_ids\"])))\n",
    "#         target_ids.append(item[\"target_ids\"] + [pad_token_id] * (MAX_LEN - len(item[\"target_ids\"])))\n",
    "\n",
    "#     # Convert to tensors\n",
    "#     return {\n",
    "#         \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "#         \"target_ids\": torch.tensor(target_ids, dtype=torch.long),\n",
    "#     }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLevelDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, mask_token_id, pad_token_id, max_length=29, device='cpu'):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.max_length = max_length\n",
    "        self.device = device \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get input and target text at the given index\n",
    "        input_text = self.inputs.iloc[idx] if isinstance(self.inputs, pd.Series) else self.inputs[idx]\n",
    "        target_text = self.targets.iloc[idx] if isinstance(self.targets, pd.Series) else self.targets[idx]\n",
    "\n",
    "        # Tokenize input and target text\n",
    "        input_ids = self.tokenizer.encode(input_text)[:self.max_length]  # Truncate to max_length\n",
    "        target_ids = self.tokenizer.encode(target_text)[:self.max_length]  # Truncate to max_length\n",
    "\n",
    "        # Convert to tensors and move to the specified device\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long, device=self.device),\n",
    "            \"labels\": torch.tensor(target_ids, dtype=torch.long, device=self.device),  # Use \"labels\" instead of \"target_ids\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_model_for_char_vocab(model_type, char_vocab):\n",
    "   \n",
    "    if model_type == \"roberta\":\n",
    "        model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "    elif model_type == \"bart\":\n",
    "        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type. Choose 'roberta' or 'bart'.\")\n",
    "\n",
    "    # Resize token embeddings to match character-level vocabulary size\n",
    "    model.resize_token_embeddings(len(char_vocab))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example input\n",
    "# input_data = [[\"_\", \"_\", \"_\"], [\"a\", \"_\", \"_\"], [\"_\", \"a\", \"_\"], [\"_\", \"_\", \"b\"], \n",
    "#               [\"a\", \"a\", \"_\"], [\"a\", \"_\", \"b\"], [\"_\", \"a\", \"b\"], [\"a\", \"a\", \"b\"]]\n",
    "\n",
    "# # Target for prediction\n",
    "# target_data = [[\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"],\n",
    "#                [\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"], [\"a\", \"a\", \"b\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==> LOADING DATASET\")\n",
    "\n",
    "\n",
    "## Load dataset [DF]\n",
    "# df = pd.read_csv(f\"./datasets/{isTEST}dataset.csv\", sep='|') \n",
    "# print(\"DATASET SHAPE : \", df.shape)\n",
    "\n",
    "# input_data  = df['input'].apply(ast.literal_eval)\n",
    "# target_data = df['target'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "# # Apply parallel processing to 'input' and 'target' columns\n",
    "# input_data, target_data = load_parallel_dataframe_apply(df, convert_to_list, n_workers=8)\n",
    "\n",
    "\n",
    "## LOAD Dataset [json]\n",
    "start = time.time()\n",
    "df = load_json_in_parallel(f\"./datasets/{isTEST}dataset.json\" , n_workers=8)\n",
    "input_data  = df.input\n",
    "target_data = df.target\n",
    "NUM_SAMPLES = df.shape[0]\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\nTIME SPENT : {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom tokenizer to tokenize by lowercase characters only\n",
    "# char_vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"[MASK]\", \"[PAD]\", \"_\"]\n",
    "\n",
    "\n",
    "# # Initialize the custom lowercase character-level tokenizer\n",
    "# char_tokenizer = CharLevelTokenizer(char_vocab)\n",
    "# mask_token_id = char_tokenizer.char_to_id[\"[MASK]\"]\n",
    "# pad_token_id = char_tokenizer.char_to_id[\"[PAD]\"]\n",
    "\n",
    "\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# train_input_data, test_input_data, train_target_data, test_target_data = train_test_split(\n",
    "#     input_data, target_data, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # Initialize the training and testing datasets\n",
    "# train_dataset = CharLevelDataset(train_input_data, train_target_data, char_tokenizer, mask_token_id)\n",
    "# test_dataset = CharLevelDataset(test_input_data, test_target_data, char_tokenizer, mask_token_id)\n",
    "\n",
    "# # train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer to tokenize by lowercase characters only\n",
    "char_vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"[MASK]\", \"[PAD]\", \"_\"]\n",
    "\n",
    "\n",
    "char_tokenizer = CharLevelTokenizer(char_vocab)\n",
    "mask_token_id = char_tokenizer.char_to_id[\"[MASK]\"]\n",
    "pad_token_id = char_tokenizer.char_to_id[\"[PAD]\"]\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_input_data, test_input_data, train_target_data, test_target_data = train_test_split(\n",
    "    input_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize dataset\n",
    "train_dataset = CharLevelDataset(train_input_data, train_target_data, char_tokenizer, mask_token_id, pad_token_id, MAX_LEN, device)\n",
    "test_dataset = CharLevelDataset(test_input_data, test_target_data, char_tokenizer, mask_token_id, pad_token_id, MAX_LEN, device)\n",
    "\n",
    "\n",
    "# Initialize dataloader\n",
    "# train_dataset_loader = DataLoader(train_dataset, batch_size=64, collate_fn=char_level_data_collator)\n",
    "# test_dataset_loader = DataLoader(test_dataset, batch_size=64, collate_fn=char_level_data_collator)\n",
    "\n",
    "\n",
    "# for batch in train_dataset_loader:  # Correctly iterates over batches\n",
    "#     print(\"Input IDs:\", batch[\"input_ids\"])\n",
    "#     print(\"Target IDs:\", batch[\"target_ids\"])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==> LOADING MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load pre-trained RoBERTa model for Masked Language Modeling (MLM)\n",
    "# model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# # Resize the model's token embeddings to match the character-level vocab size\n",
    "# model.resize_token_embeddings(len(char_tokenizer.char_vocab))  # Resize for lowercase char-level tokens\n",
    "\n",
    "model = prepare_model_for_char_vocab(MODEL_NAME, char_tokenizer.char_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Move model to MPS\n",
    "model.to(device)\n",
    "\n",
    "# Setup the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'.models/model-{MODEL_NAME}-{VERSION}/results', \n",
    "    evaluation_strategy=\"steps\", \n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=min(128, int(NUM_SAMPLES/4)), \n",
    "    logging_dir=f'.models/model-{MODEL_NAME}-{VERSION}/logs', \n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=char_tokenizer,  \n",
    "    # data_collator=None,  \n",
    ")\n",
    "\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(f\".models/model-{MODEL_NAME}-{VERSION}/fine_tuned_{MODEL_NAME}\")\n",
    "print(\"\\n==> MODEL SAVED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_characters(input_sequence, tokenizer, model, mask_token_id):\n",
    "    # Convert the input sequence to token IDs using the tokenizer\n",
    "    input_ids = tokenizer.encode(input_sequence)\n",
    "\n",
    "    # Convert input_ids to tensor and move it to the right device\n",
    "    input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "    # Run the model to predict masked token positions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Extract the predicted token IDs for each masked position\n",
    "    predicted_ids = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Convert predicted IDs to characters using the tokenizer\n",
    "    predicted_sequence = tokenizer.decode(predicted_ids)\n",
    "    return predicted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFinetunedModel(model_name , model_path = None):\n",
    "\n",
    "    if model_path and model_name == \"roberta\":\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "    elif model_path and model_name == \"bart\":\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    else:\n",
    "        print(\"ENTER VALID MODEL PATH!!\")\n",
    "        raise ValueError(\"Unsupported model type. Choose 'roberta' or 'bart'.\")\n",
    "\n",
    "\n",
    "    # Set the model to evaluation mode for inference\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\".models/model-{MODEL_NAME}-{VERSION}/fine_tuned_{MODEL_NAME}\"\n",
    "model = loadFinetunedModel(MODEL_NAME, model_path)\n",
    "\n",
    "# Load the tokenizer (same tokenizer used during training)\n",
    "char_tokenizer = CharLevelTokenizer(char_vocab)\n",
    "mask_token_id = char_tokenizer.char_to_id[\"[MASK]\"]\n",
    "pad_token_id = char_tokenizer.char_to_id[\"[PAD]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example masked input\n",
    "print(\"\\n==> TESTNG INFERENCE\")\n",
    "masked_input = [\"_\", \"a\", \"_\", \"s\"]\n",
    "\n",
    "answer = \"aahs\"\n",
    "\n",
    "\n",
    "# Call the prediction function\n",
    "predicted_output = predict_masked_characters(masked_input, char_tokenizer, model, mask_token_id)\n",
    "\n",
    "print(f\"Predicted Output: {predicted_output}\\nANSWER: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
