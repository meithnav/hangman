{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import shutil\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 31\n",
    "\n",
    "PAD_TOKEN = '[PAD]'\n",
    "MASK_TOKEN = '_'\n",
    "BOS_TOKEN = \"[START]\"\n",
    "EOS_TOKEN = \"[END]\"\n",
    "\n",
    "# Define special tokens\n",
    "SPECIAL_TOKENS = {\"pad_token\": PAD_TOKEN, \"bos_token\": BOS_TOKEN, \"eos_token\":EOS_TOKEN, 'mask_token':MASK_TOKEN }\n",
    "\n",
    "\n",
    "# isTEST='test-'\n",
    "# isTEST='new-'\n",
    "isTEST=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocabulary(file_path):\n",
    "  \n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read all lines, strip whitespace, and filter out empty lines\n",
    "        vocabulary = [line.strip() for line in file if line.strip()]\n",
    "    return vocabulary \n",
    "\n",
    "\n",
    "def clearFile(path):\n",
    "    # Remove the output file if it already exists\n",
    "    if os.path.isfile(path):  # Delete if it's a file\n",
    "        os.remove(path)\n",
    "        print(f\"File {path} has been deleted.\")\n",
    "\n",
    "\n",
    "def delete_intermediate_directory(intermediate_dir):\n",
    "    \"\"\"Delete the entire intermediate directory and its contents.\"\"\"\n",
    "    try:\n",
    "        shutil.rmtree(intermediate_dir)  # Remove the directory and all its contents\n",
    "        print(f\"Deleted intermediate directory: {intermediate_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting directory {intermediate_dir}: {e}\")\n",
    "\n",
    "\n",
    "def load_and_convert_json_with_padding(json_file, max_len=MAX_LEN, token=SPECIAL_TOKENS ):\n",
    "    \"\"\"Load a JSON file with stringified JSON objects, pad the 'input' and 'target' to max_len, and convert to DataFrame.\"\"\"\n",
    "    print(\"\\nPadding dataset\")\n",
    "   \n",
    "    with open(json_file, 'r') as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            record = json.loads(line)  # Parse each line into a Python dictionary\n",
    "            \n",
    "            # Pad the 'input' list to the desired length with start and end tokens\n",
    "            input_list = record[\"input\"] \n",
    "            padded_input = [token['bos_token']]  + input_list + [token['eos_token']] + [token['pad_token']] * (max_len - len(input_list) - 2 ) # -2 for start and end token\n",
    "            padded_input = padded_input[:max_len]  # Ensure exactly max_len long\n",
    "            \n",
    "            # Convert 'target' to a character-level list and pad\n",
    "            target_list = list(record[\"target\"]) \n",
    "            padded_target = [token['bos_token']] +  target_list + [token['eos_token']] + [token['pad_token']] * (max_len - len(target_list) -2)\n",
    "            # padded_target = [token['bos_token']] + target_list + [token['eos_token']] + [token['pad_token']] * (max_len - len(target_list) -2)\n",
    "            padded_target = padded_target[:max_len]  # Ensure exactly max_len long\n",
    "            \n",
    "            # print(len(padded_input), len(padded_target))\n",
    "            \n",
    "            # Append the processed data to the list\n",
    "            data.append({\"input\": padded_input, \"target\": padded_target})\n",
    "    \n",
    "    \n",
    "    clearFile(json_file)\n",
    "    \n",
    "    with open(f'{json_file}', 'w') as js_file:\n",
    "        json.dump(data, js_file)\n",
    "        \n",
    "    \n",
    "    # Convert the list of dicts into a Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    # print(json_file)\n",
    "    df.to_csv(f\"{json_file.split('.')[0]}.csv\", sep='|', index=False)\n",
    "    \n",
    "\n",
    "def convert_json_to_parquet_in_directory(directory_path):\n",
    "\n",
    "    try:\n",
    "        # Check if the directory exists\n",
    "        if not os.path.isdir(directory_path):\n",
    "            print(f\"The directory '{directory_path}' does not exist.\")\n",
    "            return\n",
    "\n",
    "        # List all JSON files in the directory\n",
    "        json_files = [file for file in os.listdir(directory_path) if file.endswith('.json')]\n",
    "\n",
    "        if not json_files:\n",
    "            print(f\"No JSON files found in the directory '{directory_path}'.\")\n",
    "            return\n",
    "\n",
    "        # Loop through all JSON files and convert them to Parquet\n",
    "        for json_file in json_files:\n",
    "            json_file_path = os.path.join(directory_path, json_file)\n",
    "            parquet_file_path = os.path.splitext(json_file_path)[0] + '.parquet'\n",
    "\n",
    "            try:\n",
    "                # Read the JSON file into a Pandas DataFrame\n",
    "                df = pd.read_json(json_file_path, orient='records', lines=True)\n",
    "                \n",
    "                # Write the DataFrame to a Parquet file\n",
    "                df.to_parquet(parquet_file_path, engine='pyarrow', index=False)\n",
    "\n",
    "                # Delete the original JSON file\n",
    "                os.remove(json_file_path)\n",
    "\n",
    "                print(f\"Converted '{json_file}' to Parquet and deleted the original JSON file.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file '{json_file}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRUTE FORCE -- ALL COMBINATIONS\n",
    "def masked_combinations(word):\n",
    "    masked_list = []\n",
    "    word_length = len(word)\n",
    "    \n",
    "    for num_visible in range(word_length + 1):\n",
    "        for visible_indices in combinations(range(word_length), num_visible):\n",
    "            masked_words = ['_'] * word_length\n",
    "            for idx in visible_indices:\n",
    "                masked_words[idx] = word[idx]\n",
    "            # masked_list.append(''.join(masked_words))\n",
    "            masked_list.append(masked_words)\n",
    "    \n",
    "    return masked_list\n",
    "\n",
    "\n",
    "\n",
    "# GENERATE TOP K MASKED SAMPLES\n",
    "def generate_random_masked_combinations(word, MAX_PERMUTATIONS=60):\n",
    "   \n",
    "    word_length = len(word)\n",
    "    masked_list = set()\n",
    "\n",
    "    MAX_PERMUTATIONS = round(min(MAX_PERMUTATIONS , 0.5*(pow(2,len(word)))))\n",
    "    for _ in range(MAX_PERMUTATIONS):\n",
    "        \n",
    "        # Randomly decide the number of visible characters\n",
    "        num_visible = random.randint(0, word_length)\n",
    "\n",
    "        # Randomly choose indices to be visible\n",
    "        visible_indices = random.sample(range(word_length), num_visible)\n",
    "\n",
    "        # Create the masked word\n",
    "        masked_word = ['_'] * word_length\n",
    "        for idx in visible_indices:\n",
    "            masked_word[idx] = word[idx]\n",
    "\n",
    "        masked_list.add(tuple(masked_word))\n",
    "    \n",
    "    # print(masked_list)\n",
    "\n",
    "    return list(masked_list)\n",
    "\n",
    "\n",
    "def makeMaskedDataset(df, PATH='./datasets/maskedCombinations'):\n",
    "    # masked_list=[]\n",
    "    big_words_idx=[]\n",
    "    MAX_WORD_LEN = 30\n",
    "\n",
    "    for idx, word in enumerate(df):\n",
    "\n",
    "        if len(word) < MAX_WORD_LEN: \n",
    "            # masked_list = masked_combinations(word)\n",
    "            masked_list = generate_random_masked_combinations(word)\n",
    "            \n",
    "            # Ensure the output directory exists\n",
    "            output_file = f'{PATH}/{word}.json'\n",
    "            os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "            with open(output_file, 'w') as json_file:\n",
    "                json.dump(masked_list, json_file)\n",
    "                masked_list = None\n",
    "                print(f\"STORED WORD : {word}\")\n",
    "        \n",
    "        \n",
    "        else: \n",
    "            big_words_idx.append(idx)\n",
    "    \n",
    "    \n",
    "    if big_words_idx:\n",
    "        with open(f'./{PATH}/bigwords-pending.json', 'w') as json_file:\n",
    "            json.dump(big_words_idx, json_file)\n",
    "            print(f\"STORED BIG WORDS IDX\")\n",
    "\n",
    "    print(\"STORED ALL WORDS !!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_files_into_batches(directory_path, batch_size):\n",
    "    \"\"\"Split files in a directory into batches.\"\"\"\n",
    "    files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.json')]\n",
    "    num_batches = math.ceil(len(files) / batch_size)\n",
    "    return [files[i * batch_size: (i + 1) * batch_size] for i in range(num_batches)]\n",
    "\n",
    "\n",
    "\n",
    "def process_batch(batch_files, batch_output_file):\n",
    "    \"\"\"Process a single batch of files and append the result to an intermediate file.\"\"\"\n",
    "    with open(batch_output_file, 'a') as out_f:  # Open in append mode\n",
    "        for file_path in batch_files:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)  # Load JSON data\n",
    "                target = os.path.splitext(os.path.basename(file_path))[0]  # Use file name as target\n",
    "                for input_data in data:  # Assuming data is a list of arrays\n",
    "                    json.dump({'input': input_data, 'target': target}, out_f)\n",
    "                    out_f.write('\\n')  # Ensure each entry is on a new line\n",
    "\n",
    "\n",
    "\n",
    "def merge_intermediate_files(intermediate_files, final_output_file):\n",
    "    \"\"\"Merge intermediate files into a single final output file.\"\"\"\n",
    "    \n",
    "    clearFile(final_output_file)\n",
    "\n",
    "    \n",
    "    # Merge valid JSON lines\n",
    "    with open(final_output_file, 'a') as out_f:\n",
    "        for file_path in intermediate_files:\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()  # Remove extra whitespace\n",
    "                    try:\n",
    "                        json_obj = json.loads(line)  # Validate JSON\n",
    "                        out_f.write(json.dumps(json_obj) + '\\n')  # Write as JSON line\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Skipping invalid JSON line: {line}\")\n",
    "\n",
    "    \n",
    "    print(f\"Merged intermediate files into {final_output_file}.\")\n",
    "\n",
    "\n",
    "def process_files_in_batches(directory_path, batch_size, intermediate_dir, final_output_file):\n",
    "    \"\"\"\n",
    "    Process files in batches, store intermediate results, and merge them into a final output file.\n",
    "    Then delete the entire intermediate directory.\n",
    "    \"\"\"\n",
    "    # Step 1: Split files into batches\n",
    "    batches = split_files_into_batches(directory_path, batch_size)\n",
    "    intermediate_files = []\n",
    "\n",
    "    # Step 2: Process each batch and append intermediate results\n",
    "    for i, batch_files in enumerate(batches):\n",
    "        batch_output_file = os.path.join(intermediate_dir, f'batch_{i}.json')\n",
    "        process_batch(batch_files, batch_output_file)\n",
    "        intermediate_files.append(batch_output_file)\n",
    "        print(f\"Processed batch {i + 1}/{len(batches)}\")\n",
    "\n",
    "    # Step 3: Merge intermediate files into the final output file\n",
    "    merge_intermediate_files(intermediate_files, final_output_file)\n",
    "    print(f\"Merged all batches into {final_output_file}\")\n",
    "\n",
    "    # Step 4: Delete the intermediate directory to free up memory\n",
    "    delete_intermediate_directory(intermediate_dir)\n",
    "    print(\"Deleted intermediate directory to free up memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = [\"a\", \"an\", \"ant\", \"bat\", \"cat\", \"me\", \"do\", \"see\", \"tree\", \"hangman\", \"meith\"]\n",
    "\n",
    "CHECKPOINT=0\n",
    "vocabulary = load_vocabulary(\"words_250000_train.txt\")\n",
    "\n",
    "if isTEST: \n",
    "    vocabulary = vocabulary[CHECKPOINT:10]\n",
    "else: \n",
    "    vocabulary = vocabulary[CHECKPOINT:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE MASKED SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = f'datasets/{isTEST}maskedCombinations-topk'\n",
    "\n",
    "\n",
    "## COMBINE FILES\n",
    "batch_size = 10000 # Number of files per batch\n",
    "directory_path = f'datasets/{isTEST}maskedCombinations-topk'\n",
    "intermediate_dir = f'datasets/{isTEST}merged-intermediates'  # Directory to store intermediate files\n",
    "final_output_file = f'datasets/{isTEST}dataset.json'    # Path for the final merged file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORED WORD : aaa\n",
      "STORED WORD : aaaaaa\n",
      "STORED WORD : aaas\n",
      "STORED WORD : aachen\n",
      "STORED WORD : aaee\n",
      "STORED WORD : aag\n",
      "STORED WORD : aahed\n",
      "STORED WORD : aahs\n",
      "STORED WORD : aal\n",
      "STORED WORD : aalesund\n",
      "STORED ALL WORDS !!\n"
     ]
    }
   ],
   "source": [
    "makeMaskedDataset(vocabulary, directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMBINE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/1\n",
      "File datasets/test-dataset.json has been deleted.\n",
      "Merged intermediate files into datasets/test-dataset.json.\n",
      "Merged all batches into datasets/test-dataset.json\n",
      "Deleted intermediate directory: datasets/test-merged-intermediates\n",
      "Deleted intermediate directory to free up memory.\n"
     ]
    }
   ],
   "source": [
    "# Ensure intermediate directory exists\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "# Process files in batches\n",
    "process_files_in_batches(directory_path, batch_size, intermediate_dir, final_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding dataset\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "29 29\n",
      "File datasets/test-dataset.json has been deleted.\n"
     ]
    }
   ],
   "source": [
    "load_and_convert_json_with_padding(final_output_file, MAX_LEN, SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAN PROCESS DATA\n",
      "\n",
      "\n",
      " ****ENDED SESSION !!*** \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() # clear GPU cache\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "print(f\"RAN PROCESS DATA\")\n",
    "\n",
    "print(\"\\n\\n ****ENDED SESSION !!*** \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "domianins",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
