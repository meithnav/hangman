{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import shutil\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, Counter, OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 29 \n",
    "PAD_TOKEN = '[PAD]'\n",
    "\n",
    "\n",
    "\n",
    "isTEST='test-'\n",
    "# isTEST=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocabulary(file_path):\n",
    "  \n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read all lines, strip whitespace, and filter out empty lines\n",
    "        vocabulary = [line.strip() for line in file if line.strip()]\n",
    "    return vocabulary \n",
    "\n",
    "\n",
    "def load_and_convert_json_with_padding(json_file, max_len=MAX_LEN, pad_token=PAD_TOKEN):\n",
    "    \"\"\"Load a JSON file with stringified JSON objects, pad the 'input' and 'target' to max_len, and convert to DataFrame.\"\"\"\n",
    "    print(\"\\nPadding dataset\")\n",
    "   \n",
    "    with open(json_file, 'r') as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            record = json.loads(line)  # Parse each line into a Python dictionary\n",
    "            \n",
    "            # Pad the 'input' list to the desired length\n",
    "            input_list = record[\"input\"]\n",
    "            padded_input = input_list + [pad_token] * (max_len - len(input_list))\n",
    "            padded_input = padded_input[:max_len]  # Ensure exactly max_len long\n",
    "            \n",
    "            # Convert 'target' to a character-level list and pad\n",
    "            target_list = list(record[\"target\"])  # Convert target to list of characters\n",
    "            padded_target = target_list + [pad_token] * (max_len - len(target_list))\n",
    "            padded_target = padded_target[:max_len]  # Ensure exactly max_len long\n",
    "            \n",
    "            # Append the processed data to the list\n",
    "            data.append({\"input\": padded_input, \"target\": padded_target})\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open(f'{json_file}', 'w') as js_file:\n",
    "        json.dump(data, js_file)\n",
    "        \n",
    "    \n",
    "    # Convert the list of dicts into a Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    # print(json_file)\n",
    "    df.to_csv(f\"{json_file.split('.')[0]}.csv\", sep='|', index=False)\n",
    "    \n",
    "\n",
    "def convert_json_to_parquet_in_directory(directory_path):\n",
    "\n",
    "    try:\n",
    "        # Check if the directory exists\n",
    "        if not os.path.isdir(directory_path):\n",
    "            print(f\"The directory '{directory_path}' does not exist.\")\n",
    "            return\n",
    "\n",
    "        # List all JSON files in the directory\n",
    "        json_files = [file for file in os.listdir(directory_path) if file.endswith('.json')]\n",
    "\n",
    "        if not json_files:\n",
    "            print(f\"No JSON files found in the directory '{directory_path}'.\")\n",
    "            return\n",
    "\n",
    "        # Loop through all JSON files and convert them to Parquet\n",
    "        for json_file in json_files:\n",
    "            json_file_path = os.path.join(directory_path, json_file)\n",
    "            parquet_file_path = os.path.splitext(json_file_path)[0] + '.parquet'\n",
    "\n",
    "            try:\n",
    "                # Read the JSON file into a Pandas DataFrame\n",
    "                df = pd.read_json(json_file_path, orient='records', lines=True)\n",
    "                \n",
    "                # Write the DataFrame to a Parquet file\n",
    "                df.to_parquet(parquet_file_path, engine='pyarrow', index=False)\n",
    "\n",
    "                # Delete the original JSON file\n",
    "                os.remove(json_file_path)\n",
    "\n",
    "                print(f\"Converted '{json_file}' to Parquet and deleted the original JSON file.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file '{json_file}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRUTE FORCE -- ALL COMBINATIONS\n",
    "def masked_combinations(word):\n",
    "    masked_list = []\n",
    "    word_length = len(word)\n",
    "    \n",
    "    for num_visible in range(word_length + 1):\n",
    "        for visible_indices in combinations(range(word_length), num_visible):\n",
    "            masked_words = ['_'] * word_length\n",
    "            for idx in visible_indices:\n",
    "                masked_words[idx] = word[idx]\n",
    "            # masked_list.append(''.join(masked_words))\n",
    "            masked_list.append(masked_words)\n",
    "    \n",
    "    return masked_list\n",
    "\n",
    "\n",
    "\n",
    "# GENERATE TOP K MASKED SAMPLES\n",
    "def generate_random_masked_combinations(word, MAX_PERMUTATIONS=60):\n",
    "   \n",
    "    word_length = len(word)\n",
    "    masked_list = set()\n",
    "\n",
    "    MAX_PERMUTATIONS = round(min(MAX_PERMUTATIONS , 0.5*(pow(2,len(word)))))\n",
    "    for _ in range(MAX_PERMUTATIONS):\n",
    "        \n",
    "        # Randomly decide the number of visible characters\n",
    "        num_visible = random.randint(0, word_length)\n",
    "\n",
    "        # Randomly choose indices to be visible\n",
    "        visible_indices = random.sample(range(word_length), num_visible)\n",
    "\n",
    "        # Create the masked word\n",
    "        masked_word = ['_'] * word_length\n",
    "        for idx in visible_indices:\n",
    "            masked_word[idx] = word[idx]\n",
    "\n",
    "        masked_list.add(tuple(masked_word))\n",
    "    \n",
    "    # print(masked_list)\n",
    "\n",
    "    return list(masked_list)\n",
    "\n",
    "\n",
    "def makeMaskedDataset(df, PATH='./datasets/maskedCombinations'):\n",
    "    # masked_list=[]\n",
    "    big_words_idx=[]\n",
    "    MAX_WORD_LEN = 30\n",
    "\n",
    "    for idx, word in enumerate(df):\n",
    "\n",
    "        if len(word) < MAX_WORD_LEN: \n",
    "            # masked_list = masked_combinations(word)\n",
    "            masked_list = generate_random_masked_combinations(word)\n",
    "            \n",
    "            # Ensure the output directory exists\n",
    "            output_file = f'{PATH}/{word}.json'\n",
    "            os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "            with open(output_file, 'w') as json_file:\n",
    "                json.dump(masked_list, json_file)\n",
    "                masked_list = None\n",
    "                print(f\"STORED WORD : {word}\")\n",
    "        \n",
    "        \n",
    "        else: \n",
    "            big_words_idx.append(idx)\n",
    "    \n",
    "    \n",
    "    if big_words_idx:\n",
    "        with open(f'./{PATH}/bigwords-pending.json', 'w') as json_file:\n",
    "            json.dump(big_words_idx, json_file)\n",
    "            print(f\"STORED BIG WORDS IDX\")\n",
    "\n",
    "    print(\"STORED ALL WORDS !!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_files_into_batches(directory_path, batch_size):\n",
    "    \"\"\"Split files in a directory into batches.\"\"\"\n",
    "    files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.json')]\n",
    "    num_batches = math.ceil(len(files) / batch_size)\n",
    "    return [files[i * batch_size: (i + 1) * batch_size] for i in range(num_batches)]\n",
    "\n",
    "def process_batch(batch_files, batch_output_file):\n",
    "    \"\"\"Process a single batch of files and append the result to an intermediate file.\"\"\"\n",
    "    with open(batch_output_file, 'a') as out_f:  # Open in append mode\n",
    "        for file_path in batch_files:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)  # Load JSON data\n",
    "                target = os.path.splitext(os.path.basename(file_path))[0]  # Use file name as target\n",
    "                for input_data in data:  # Assuming data is a list of arrays\n",
    "                    json.dump({'input': input_data, 'target': target}, out_f)\n",
    "                    out_f.write('\\n')  # Ensure each entry is on a new line\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_intermediate_files(intermediate_files, final_output_file):\n",
    "    \"\"\"Merge intermediate files into a single final output file, appending data.\"\"\"\n",
    "    with open(final_output_file, 'a') as out_f:  # Open in append mode\n",
    "        for file_path in intermediate_files:\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    out_f.write(line)\n",
    "\n",
    "def delete_intermediate_directory(intermediate_dir):\n",
    "    \"\"\"Delete the entire intermediate directory and its contents.\"\"\"\n",
    "    try:\n",
    "        shutil.rmtree(intermediate_dir)  # Remove the directory and all its contents\n",
    "        print(f\"Deleted intermediate directory: {intermediate_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting directory {intermediate_dir}: {e}\")\n",
    "\n",
    "def process_files_in_batches(directory_path, batch_size, intermediate_dir, final_output_file):\n",
    "    \"\"\"\n",
    "    Process files in batches, store intermediate results, and merge them into a final output file.\n",
    "    Then delete the entire intermediate directory.\n",
    "    \"\"\"\n",
    "    # Step 1: Split files into batches\n",
    "    batches = split_files_into_batches(directory_path, batch_size)\n",
    "    intermediate_files = []\n",
    "\n",
    "    # Step 2: Process each batch and append intermediate results\n",
    "    for i, batch_files in enumerate(batches):\n",
    "        batch_output_file = os.path.join(intermediate_dir, f'batch_{i}.json')\n",
    "        process_batch(batch_files, batch_output_file)\n",
    "        intermediate_files.append(batch_output_file)\n",
    "        print(f\"Processed batch {i + 1}/{len(batches)}\")\n",
    "\n",
    "    # Step 3: Merge intermediate files into the final output file\n",
    "    merge_intermediate_files(intermediate_files, final_output_file)\n",
    "    print(f\"Merged all batches into {final_output_file}\")\n",
    "\n",
    "    # Step 4: Delete the intermediate directory to free up memory\n",
    "    delete_intermediate_directory(intermediate_dir)\n",
    "    print(\"Deleted intermediate directory to free up memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = [\"a\", \"an\", \"ant\", \"bat\", \"cat\", \"me\", \"do\", \"see\", \"tree\", \"hangman\", \"meith\"]\n",
    "\n",
    "CHECKPOINT=0\n",
    "vocabulary = load_vocabulary(\"words_250000_train.txt\")\n",
    "\n",
    "if isTEST: \n",
    "    vocabulary = vocabulary[CHECKPOINT:10]\n",
    "else: \n",
    "    vocabulary = vocabulary[CHECKPOINT:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE MASKED SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = f'datasets/{isTEST}maskedCombinations-topk'\n",
    "\n",
    "\n",
    "## COMBINE FILES\n",
    "batch_size = 10000 # Number of files per batch\n",
    "directory_path = f'datasets/{isTEST}maskedCombinations-topk'\n",
    "intermediate_dir = f'datasets/{isTEST}merged-intermediates'  # Directory to store intermediate files\n",
    "final_output_file = f'datasets/{isTEST}dataset.json'    # Path for the final merged file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORED WORD : aaa\n",
      "STORED WORD : aaaaaa\n",
      "STORED WORD : aaas\n",
      "STORED WORD : aachen\n",
      "STORED WORD : aaee\n",
      "STORED WORD : aag\n",
      "STORED WORD : aahed\n",
      "STORED WORD : aahs\n",
      "STORED WORD : aal\n",
      "STORED WORD : aalesund\n",
      "STORED ALL WORDS !!\n"
     ]
    }
   ],
   "source": [
    "makeMaskedDataset(vocabulary, directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMBINE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/1\n",
      "Merged all batches into datasets/test-dataset.json\n",
      "Deleted intermediate directory: datasets/test-merged-intermediates\n",
      "Deleted intermediate directory to free up memory.\n"
     ]
    }
   ],
   "source": [
    "# Ensure intermediate directory exists\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "# Process files in batches\n",
    "process_files_in_batches(directory_path, batch_size, intermediate_dir, final_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding dataset\n"
     ]
    }
   ],
   "source": [
    "load_and_convert_json_with_padding(final_output_file, MAX_LEN, PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
